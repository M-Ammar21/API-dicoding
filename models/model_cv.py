# -*- coding: utf-8 -*-
"""CV_skill_extractor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BQp1b-dO-wWyVXnlPuB5E5oiTMLlEb-s
"""

# !pip install -q pdfminer.six
# !pip install -q docx2txt

# from google.colab import drive
import json
import sys
from dotenv import load_dotenv
from pdfminer.high_level import extract_text
import docx2txt
import nltk
from pymongo import MongoClient
nltk.download('stopwords')
nltk.download('punkt')
import os
import pandas as pd

def extract_text_from_pdf(pdf_path):
    return extract_text(pdf_path)

def extract_text_from_docx(docx_path):
    txt = docx2txt.process(docx_path)
    if txt:
        return txt.replace('\t', ' ')
    return None

def add_dict(skill, input) :
  if input in skill :
    skill[input]+=1
  else :
    skill[input]=1

def extract_skills(input_text, n):
    stop_words = set(nltk.corpus.stopwords.words('english'))
    word_tokens = nltk.tokenize.word_tokenize(input_text)

    #remove stopwords
    filtered_tokens = [w for w in word_tokens if w not in stop_words]

    #capure bigrams_trigrams(terms constructed from 2 or 3 words) such as Machine Learning or Machine Learning Developer
    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))

    #set to ensure no duplicated skill
    skills = {}

    #search through each filtered words
    for token in filtered_tokens:
        if token.lower() in SKILLS_DB:
            add_dict(skills, token.lower())

    #search through each bigrams_trigrams
    for ngram in bigrams_trigrams:
        if ngram.lower() in SKILLS_DB:
            add_dict(skills, ngram.lower())

    return sorted(skills.items(), key=lambda item: item[1], reverse=True)

def check_file_type(file_path):
    _, file_extension = os.path.splitext(file_path)
    if file_extension.lower() == '.docx':
        return 'docx'
    elif file_extension.lower() == '.pdf':
        return 'pdf'
    else:
        return 'unknown'

def CV_extract(path, n) :
  file_type = check_file_type(path)
  if file_type == 'pdf' :
    text = extract_text_from_pdf(path)
  else :
    text = extract_text_from_docx(path)
  skills = extract_skills(text, n)
  return list(dict(skills[:n]).keys())

#list for skills available(in lowercase)
load_dotenv()
mongodb_uri = os.getenv("MONGODB_URI")
client = MongoClient(mongodb_uri)
db = client['capstone']
collection = db['bang']
data = list(collection.find({}, {'skill': 1}))
# print(data)

SKILLS_DB = [d['skill'] for d in data]

# print(SKILLS_DB)


if __name__ == "__main__":
    file_path = sys.argv[1]  # Path file yang diterima dari argumen command line
    n = int(sys.argv[2])  # Jumlah top skill yang ingin diekstraksi
    # n = 3
    skills = CV_extract(file_path, n)
    # Output skills sebagai JSON
    print(json.dumps({"jobs": skills}))
# SKILLS_DB = pd.read_csv('/content/SKILLS_DB - Sheet1.csv', names = ['skills'])
# SKILLS_DB = list(SKILLS_DB['skills'])

#input :
# cv file (docx or pdf)
# n : Top n skill
# CV_extract('/content/CV.docx', 3)
